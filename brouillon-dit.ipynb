{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senegal = df[df['Country'] == 'Senegal']\n",
    "rest_grouped = senegal.groupby('Date')['Recovered', 'Dead', 'Active'].sum().reset_index()\n",
    "\n",
    "temp = rest_grouped.melt(id_vars=\"Date\", value_vars=['Recovered', 'Dead', 'Active'],\n",
    "                 var_name='case', value_name='count')\n",
    "\n",
    "\n",
    "fig = px.area(temp, x=\"Date\", y=\"count\", color='case',\n",
    "             title='Cases - Senegal: Area Plot', color_discrete_sequence = ['cyan', 'red', 'orange'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data senegal\n",
    "df_all = pd.read_csv(\"COVID_Senegal.csv\",sep=\";\" )\n",
    "#df['Date'].unique()\n",
    "\n",
    "#sorted(df['Date'], key=lambda d: map(int, d.split('-')))\n",
    "drop_clo = ['Age','Age', 'Homme', 'Femme','Nationalité', 'Resident Senegal', 'Ville', 'Facteur','Source/Voyage', 'Hopital', 'Temps Hospitalisation (j)']\n",
    "df=df_all.drop(drop_clo,axis=1)\n",
    "df.columns\n",
    "df.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed_df = df[df['Positif']==1]\n",
    "confirmed = confirmed_df[['Date', 'Positif']]\n",
    "confirmed_cases= confirmed[['Date', 'Positif']].groupby(\"Date\").sum()\n",
    "confirmed_cases.head()\n",
    "\n",
    "deaths_df = df[df['Décédé']==1]\n",
    "deaths = deaths_df[['Date', 'Décédé']]\n",
    "deaths_cases= deaths[['Date', 'Décédé']].groupby(\"Date\").sum()\n",
    "\n",
    "recoveries_df = df[df['Guéri']==1]\n",
    "recoveries = recoveries_df[['Date', 'Guéri']]\n",
    "recoveries_cases= recoveries[['Date', 'Guéri']].groupby(\"Date\").sum()\n",
    "\n",
    "\n",
    "negatif_df = df[df['Negatif']==1  ]\n",
    "negatif = negatif_df[['Date', 'Negatif']]\n",
    "negatif_cases=negatif[['Date', 'Negatif']].groupby(\"Date\").sum()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test1 = pd.DataFrame(confirmed_cases)\n",
    "test2 = pd.DataFrame(negatif_cases)\n",
    "total_cases = pd.concat([test1, test2], axis=1)\n",
    "total_cases = total_cases.fillna(0)\n",
    "total_cases['Negatif']\n",
    "total = total_cases['Positif'] + total_cases['Negatif']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "confirmed_cases = confirmed_cases.reset_index()\n",
    "confirmed_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a row which we want to insert \n",
    "#row_number = 4\n",
    "row_value = ['05.03.20', 0] \n",
    "row_value1 = ['06.03.20', 0] \n",
    "row_value2 = ['07.03.20', 0] \n",
    "row_value3 = ['08.03.20', 0] \n",
    "row_value4 = ['09.03.20', 0] \n",
    "row_value5 = ['10.03.20', 0] \n",
    "\n",
    "# Let's call the function and insert the row \n",
    "    # at the second position \n",
    "df_clean = Insert_row_( 4, confirmed_cases, row_value) \n",
    "df_clean1 = Insert_row_( 5, df_clean, row_value1) \n",
    "df_clean2 = Insert_row_( 6, df_clean1, row_value2) \n",
    "df_clean3 = Insert_row_( 7, df_clean2, row_value3) \n",
    "df_clean4 = Insert_row_( 8, df_clean3, row_value4) \n",
    "df_clean5 = Insert_row_( 9, df_clean4, row_value5) \n",
    "\n",
    "\n",
    "confirmed_cases= df_clean5\n",
    "# set column 'Name' as the index of the Dataframe\n",
    "#confirmed_cases = confirmed_cases.set_index('Date')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Print the updated dataframe \n",
    "print(confirmed_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed_cases['Date'] =confirmed_cases.to_datetime(confirmed_cases.Date)\n",
    "confirmed_cases.sort('Date') # This now sorts in date order\n",
    "\n",
    "#confirmed_cases.sort_values(by='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_each_day =  confirmed_cases\n",
    "data_cum      = confirmed_cases.cumsum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_since_each = np.array([i for i in range(len(data_each_day))]).reshape(-1, 1)\n",
    "cases_each = np.array(data_each_day).reshape(-1, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_since_cum = np.array([i for i in range(len(data_cum))]).reshape(-1, 1)\n",
    "cases_cum = np.array(data_cum).reshape(-1, 1)\n",
    "len(cases_cum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 30  # Nombre de jours de predictions\n",
    "days_in_future = n\n",
    "future_forcast = np.array([i for i in range(len(confirmed_cases)+days_in_future)]).reshape(-1, 1)\n",
    "adjusted_dates = future_forcast[:-n]\n",
    "adjusted_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = '02/03/2020'   # date debut de l'epidemie\n",
    "start_date = datetime.datetime.strptime(start, '%d/%m/%Y')\n",
    "print(start_date)\n",
    "future_forcast_dates = []\n",
    "for i in range(len(future_forcast)):\n",
    "    future_forcast_dates.append((start_date + datetime.timedelta(days=i)).strftime('%d/%m/%Y'))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_each, X_test_each, y_train_each, y_test_each = train_test_split(days_since_each, cases_each, test_size=0.25, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cum, X_test_cum, y_train_cum, y_test_cum = train_test_split(days_since_cum, cases_cum, test_size=0.25, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bayesian ridge \n",
    "tol = [1e-4, 1e-3, 1e-2]\n",
    "alpha_1 = [1e-7, 1e-6, 1e-5, 1e-4]\n",
    "alpha_2 = [1e-7, 1e-6, 1e-5, 1e-4]\n",
    "lambda_1 = [1e-7, 1e-6, 1e-5, 1e-4]\n",
    "lambda_2 = [1e-7, 1e-6, 1e-5, 1e-4]\n",
    "\n",
    "bayesian_grid = {'tol': tol, 'alpha_1': alpha_1, 'alpha_2' : alpha_2, 'lambda_1': lambda_1, 'lambda_2' : lambda_2}\n",
    "\n",
    "bayesian = BayesianRidge(compute_score=True)\n",
    "bayesian_search = RandomizedSearchCV(bayesian, bayesian_grid, scoring='neg_mean_squared_error', cv=3, return_train_score=True, n_jobs=-1, n_iter=40, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_search.fit(X_train_each, y_train_each)\n",
    "\n",
    "bayesian_search.best_params_\n",
    "\n",
    "bayesian_each = bayesian_search.best_estimator_\n",
    "each_bayesian_pred = bayesian_each.predict(X_test_each)\n",
    "print('MAE:', mean_absolute_error(each_bayesian_pred, y_test_each))\n",
    "print('MSE:',mean_squared_error(each_bayesian_pred, y_test_each))\n",
    "\n",
    "len(each_bayesian_pred)\n",
    "\n",
    "each_bayesian_pred_future = bayesian_each.predict(future_forcast)\n",
    "each_bayesian_pred_future\n",
    "\n",
    "# Future predictions using Linear Regression \n",
    "print('Ridge regression future predictions:')\n",
    "set(zip(future_forcast_dates[28:35], np.round(each_bayesian_pred_future[28:35]) ))\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.plot(adjusted_dates, cases_each)\n",
    "plt.plot(future_forcast_dates, each_bayesian_pred_future, linestyle='dashed', color='green')\n",
    "plt.title('Nombre de cas confirmes en fonction du temps', size=30)\n",
    "plt.xlabel('Temps', size=30)\n",
    "plt.ylabel('Nombre de cas', size=30)\n",
    "plt.legend([' Cas Confirmes', 'Bayesian Ridge Regression Predictions'], prop={'size': 20})\n",
    "plt.xticks(size=20, rotation= 90)\n",
    "plt.yticks(size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_search.fit(X_train_cum, y_train_cum)\n",
    "\n",
    "bayesian_cum = bayesian_search.best_estimator_\n",
    "cum_bayesian_pred = bayesian_each.predict(X_test_cum)\n",
    "print('MAE:', mean_absolute_error(cum_bayesian_pred, y_test_cum))\n",
    "print('MSE:',mean_squared_error(cum_bayesian_pred, y_test_cum))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cum_bayesian_pred_future = bayesian_cum.predict(future_forcast)\n",
    "len(cum_bayesian_pred_future)\n",
    "\n",
    "# Future predictions using Linear Regression \n",
    "print('Ridge regression future predictions:')\n",
    "set(zip(future_forcast_dates, np.round(cum_bayesian_pred_future) ))\n",
    "\n",
    "\n",
    "adjusted_dates\n",
    "future_forcast_dates[1:len(cases_cum)]\n",
    "len(cases_cum)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.plot(adjusted_dates, cases_cum, color='blue')\n",
    "plt.plot(future_forcast_dates, cum_bayesian_pred_future, linestyle='dashed', color='green')\n",
    "plt.title('Nombre de cas confirmes en fonction du temps', size=30)\n",
    "plt.xlabel('Temps', size=20)\n",
    "plt.ylabel('Nombre de cas', size=30)\n",
    "plt.legend([' Cas Confirmes', 'Bayesian Ridge Regression Predictions'], prop={'size': 30})\n",
    "plt.xticks(size=20, rotation = 90)\n",
    "plt.yticks(size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial Regression\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "p = 2\n",
    "poly = PolynomialFeatures(degree = p)\n",
    "X_poly= poly.fit_transform(X_train_cum)\n",
    "poly.fit(X_poly, y_train_cum)\n",
    "lin2 = LinearRegression()\n",
    "lin2.fit(X_poly, y_train_cum)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(X_train_cum,y_train_cum, color ='blue')\n",
    "plt.plot(X_train_cum, lin2.predict(poly.fit_transform(X_train_cum)), color ='red')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "lin2.predict(poly.fit_transform(X_test_cum))\n",
    "\n",
    "cum_poly_pred_future = lin2.predict(poly.fit_transform(future_forcast))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.plot(future_forcast_dates[0:len(cases_cum)], cases_cum, color='blue')\n",
    "plt.plot(future_forcast_dates, cum_poly_pred_future, linestyle='dashed', color='green')\n",
    "plt.title('Nombre de cas confirmes en fonction du temps', size=30)\n",
    "plt.xlabel('Temps', size=20)\n",
    "plt.ylabel('Nombre de cas', size=30)\n",
    "plt.legend([' Cas Confirmes', 'polynomial Regression Predictions'], prop={'size': 30})\n",
    "plt.xticks(size=20, rotation = 90)\n",
    "plt.yticks(size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling Logistic Growth\n",
    "\n",
    "Reference\n",
    "\n",
    "https://jooskorstanje.com/modeling-logistic-growth-corona.html\n",
    "https://towardsdatascience.com/modeling-exponential-growth-49a2b6f22e1f\n",
    "https://towardsdatascience.com/modeling-exponential-growth-49a2b6f22e1f\n",
    "https://towardsdatascience.com/modeling-logistic-growth-1367dc971de2\n",
    "\n",
    "\n",
    "La croissance logistique est une fonction mathématique qui peut être utilisée dans plusieurs situations. La croissance logistique se caractérise par une croissance croissante au début, mais une croissance décroissante à un stade ultérieur, à mesure que l'on se rapproche d'un maximum. Par exemple, dans le cas du coronavirus, cette limite maximale serait le nombre total de personnes dans le monde, car lorsque tout le monde est malade, la croissance diminuera nécessairement.\n",
    "Dans d'autres cas d'utilisation de croissance logistique, ce nombre pourrait être la taille d'une population animale qui croît de façon exponentielle jusqu'au moment où leur environnement ne fournit pas suffisamment de nourriture pour tous les animaux et donc la croissance devient plus lente jusqu'à ce qu'une capacité maximale de l'environnement soit atteinte .\n",
    "La raison d'utiliser la croissance logistique pour modéliser l'épidémie de coronavirus est que les épidémiologistes ont étudié ces types d'épidémies et il est bien connu que la première période d'une épidémie suit la croissance exponentielle et que la période totale peut être modélisée avec une croissance logistique.\n",
    "\n",
    "# Define funcion with the coefficients to estimate\n",
    "def my_logistic(t, a, b, c):\n",
    "    return c / (1 + a * np.exp(-b*t))\n",
    "\n",
    "# Randomly initialize the coefficients\n",
    "p0 = np.random.exponential(size=3)\n",
    "p0\n",
    "\n",
    "# Set min bound 0 on all coefficients, and set different max bounds for each coefficient\n",
    "bounds = (0, [100, 3., 1000])\n",
    "\n",
    "data_cum\n",
    "data = data_cum.reset_index(drop=False)\n",
    "data= data.reset_index(drop=False)\n",
    "data.head()\n",
    "\n",
    "#data = data[['index', 'Daily_case']]\n",
    "data = data[['index', 'Positif']]\n",
    "\n",
    "\n",
    "data.columns = ['Timestep', 'Total Cases']\n",
    "\n",
    "# Convert pd.Series to np.Array and use Scipy's curve fit to find the best Nonlinear Least Squares coefficients\n",
    "x = np.array(data['Timestep']) + 1\n",
    "y = np.array(data['Total Cases'])\n",
    "\n",
    "(a,b,c),cov = optim.curve_fit(my_logistic, x, y, bounds=bounds, p0=p0)\n",
    "\n",
    "\n",
    "# Show the coefficients\n",
    "a,b,c\n",
    "\n",
    "# Redefine the function with the new a, b and c\n",
    "def my_logistic(t):\n",
    "    return c / (1 + a * np.exp(-b*t))\n",
    "\n",
    "plt.figure(figsize=(20,12))\n",
    "plt.scatter(future_forcast_dates[0:len(y)], y)\n",
    "plt.plot(x, my_logistic(x),color = 'red')\n",
    "plt.title('Logistic Model vs Real Observations of Senegal Coronavirus')\n",
    "plt.legend([ 'Logistic Model', 'Real data'])\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Infections')\n",
    "plt.xticks(rotation = 90)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20,12))\n",
    "plt.scatter(x, y)\n",
    "plt.plot(future_forcast_dates, my_logistic(future_forcast),color = 'red')\n",
    "plt.title('Logistic Model vs Real Observations of Senegal Coronavirus')\n",
    "plt.legend([ ' Predict Logistic Model ', 'Real data'])\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Infections')\n",
    "plt.xticks(rotation = 90)\n",
    "plt.show()\n",
    "\n",
    "Calcul du moment de croissance rapide\n",
    "\n",
    "# The time step at which the growth is fastest\n",
    "t_fastest = np.log(a) / b\n",
    "t_fastest\n",
    "print(np.round(t_fastest))\n",
    "future_forcast_dates[27]\n",
    "\n",
    "# First way to find the y of the fastest growth moment\n",
    "y_fastest = c / 2\n",
    "y_fastest\n",
    "\n",
    "# Second way to find the y of the fastest growth moment\n",
    "my_logistic(t_fastest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "print(len(train), len(test))\n",
    "\n",
    " #convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back-1):\n",
    "\t\ta = dataset[i:(i+look_back), 0]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back, 0])\n",
    "\treturn numpy.array(dataX), numpy.array(dataY)\n",
    "\n",
    "# reshape into X=t and Y=t+1\n",
    "look_back = 1\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "\n",
    "\n",
    "\n",
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(1, look_back)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)\n",
    "\n",
    "# make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()\n",
    "\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back-1):\n",
    "\t\ta = dataset[i:(i+look_back), 0]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back, 0])\n",
    "\treturn numpy.array(dataX), numpy.array(dataY)\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load the dataset\n",
    "#dataframe = read_csv('airline-passengers.csv', usecols=[1], engine='python')\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "# reshape into X=t and Y=t+1\n",
    "look_back = 3\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(1, look_back)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)\n",
    "# make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()\n",
    "\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back-1):\n",
    "\t\ta = dataset[i:(i+look_back), 0]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back, 0])\n",
    "\treturn numpy.array(dataX), numpy.array(dataY)\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load the dataset\n",
    "#dataframe = read_csv('airline-passengers.csv', usecols=[1], engine='python')\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "# reshape into X=t and Y=t+1\n",
    "look_back = 3\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "testX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(look_back, 1)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)\n",
    "# make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()\n",
    "\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back-1):\n",
    "\t\ta = dataset[i:(i+look_back), 0]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back, 0])\n",
    "\treturn numpy.array(dataX), numpy.array(dataY)\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load the dataset\n",
    "#dataframe = read_csv('airline-passengers.csv', usecols=[1], engine='python')\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "# reshape into X=t and Y=t+1\n",
    "look_back = 3\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "testX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "# create and fit the LSTM network\n",
    "batch_size = 1\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "for i in range(100):\n",
    "\tmodel.fit(trainX, trainY, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)\n",
    "\tmodel.reset_states()\n",
    "# make predictions\n",
    "trainPredict = model.predict(trainX, batch_size=batch_size)\n",
    "model.reset_states()\n",
    "testPredict = model.predict(testX, batch_size=batch_size)\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()\n",
    "\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back-1):\n",
    "\t\ta = dataset[i:(i+look_back), 0]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back, 0])\n",
    "\treturn numpy.array(dataX), numpy.array(dataY)\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load the dataset\n",
    "#dataframe = read_csv('airline-passengers.csv', usecols=[1], engine='python')\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "# reshape into X=t and Y=t+1\n",
    "look_back = 3\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "testX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "# create and fit the LSTM network\n",
    "batch_size = 1\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True, return_sequences=True))\n",
    "model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "for i in range(100):\n",
    "\tmodel.fit(trainX, trainY, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)\n",
    "\tmodel.reset_states()\n",
    "# make predictions\n",
    "trainPredict = model.predict(trainX, batch_size=batch_size)\n",
    "model.reset_states()\n",
    "testPredict = model.predict(testX, batch_size=batch_size)\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()\n",
    "\n",
    "from pandas import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "# load dataset\n",
    "def parser(x):\n",
    "\treturn datetime.strptime('190'+x, '%Y-%m')\n",
    "#series = read_csv('shampoo-sales.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\n",
    "# split data into train and test\n",
    "X = dataframe.values\n",
    "train, test = X[0:-12], X[-12:]\n",
    "# walk-forward validation\n",
    "history = [x for x in train]\n",
    "predictions = list()\n",
    "for i in range(len(test)):\n",
    "\t# make prediction\n",
    "\tpredictions.append(history[-1])\n",
    "\t# observation\n",
    "\thistory.append(test[i])\n",
    "# report performance\n",
    "rmse = sqrt(mean_squared_error(test, predictions))\n",
    "print('RMSE: %.3f' % rmse)\n",
    "# line plot of observed vs predicted\n",
    "pyplot.plot(test)\n",
    "pyplot.plot(predictions)\n",
    "pyplot.show()\n",
    "\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "import numpy\n",
    " \n",
    "# date-time parsing function for loading the dataset\n",
    "def parser(x):\n",
    "\treturn datetime.strptime('190'+x, '%Y-%m')\n",
    " \n",
    "# frame a sequence as a supervised learning problem\n",
    "def timeseries_to_supervised(data, lag=1):\n",
    "\tdf = DataFrame(data)\n",
    "\tcolumns = [df.shift(i) for i in range(1, lag+1)]\n",
    "\tcolumns.append(df)\n",
    "\tdf = concat(columns, axis=1)\n",
    "\tdf.fillna(0, inplace=True)\n",
    "\treturn df\n",
    " \n",
    "# create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "\tdiff = list()\n",
    "\tfor i in range(interval, len(dataset)):\n",
    "\t\tvalue = dataset[i] - dataset[i - interval]\n",
    "\t\tdiff.append(value)\n",
    "\treturn Series(diff)\n",
    " \n",
    "# invert differenced value\n",
    "def inverse_difference(history, yhat, interval=1):\n",
    "\treturn yhat + history[-interval]\n",
    " \n",
    "# scale train and test data to [-1, 1]\n",
    "def scale(train, test):\n",
    "\t# fit scaler\n",
    "\tscaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\tscaler = scaler.fit(train)\n",
    "\t# transform train\n",
    "\ttrain = train.reshape(train.shape[0], train.shape[1])\n",
    "\ttrain_scaled = scaler.transform(train)\n",
    "\t# transform test\n",
    "\ttest = test.reshape(test.shape[0], test.shape[1])\n",
    "\ttest_scaled = scaler.transform(test)\n",
    "\treturn scaler, train_scaled, test_scaled\n",
    " \n",
    "# inverse scaling for a forecasted value\n",
    "def invert_scale(scaler, X, value):\n",
    "\tnew_row = [x for x in X] + [value]\n",
    "\tarray = numpy.array(new_row)\n",
    "\tarray = array.reshape(1, len(array))\n",
    "\tinverted = scaler.inverse_transform(array)\n",
    "\treturn inverted[0, -1]\n",
    " \n",
    "# fit an LSTM network to training data\n",
    "def fit_lstm(train, batch_size, nb_epoch, neurons):\n",
    "\tX, y = train[:, 0:-1], train[:, -1]\n",
    "\tX = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "\tmodel.add(Dense(1))\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\tfor i in range(nb_epoch):\n",
    "\t\tmodel.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n",
    "\t\tmodel.reset_states()\n",
    "\treturn model\n",
    " \n",
    "# make a one-step forecast\n",
    "def forecast_lstm(model, batch_size, X):\n",
    "\tX = X.reshape(1, 1, len(X))\n",
    "\tyhat = model.predict(X, batch_size=batch_size)\n",
    "\treturn yhat[0,0]\n",
    " \n",
    "# load dataset\n",
    "#series = read_csv('shampoo-sales.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\n",
    " \n",
    "# transform data to be stationary\n",
    "raw_values = dataframe.values\n",
    "diff_values = difference(raw_values, 10)\n",
    " \n",
    "# transform data to be supervised learning\n",
    "supervised = timeseries_to_supervised(diff_values, 1)\n",
    "supervised_values = supervised.values\n",
    "\n",
    "supervised\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "# split data into train and test-sets\n",
    "train, test = supervised_values[0:-12], supervised_values[-12:]\n",
    " \n",
    "# transform the scale of the data\n",
    "scaler, train_scaled, test_scaled = scale(train, test)\n",
    " \n",
    "# fit the model\n",
    "lstm_model = fit_lstm(train_scaled, 1, 200, 1)\n",
    "# forecast the entire training dataset to build up state for forecasting\n",
    "train_reshaped = train_scaled[:, 0].reshape(len(train_scaled), 1, 1)\n",
    "lstm_model.predict(train_reshaped, batch_size=1)\n",
    " \n",
    "# walk-forward validation on the test data\n",
    "predictions = list()\n",
    "for i in range(len(test_scaled)):\n",
    "\t# make one-step forecast\n",
    "\tX, y = test_scaled[i, 0:-1], test_scaled[i, -1]\n",
    "\tyhat = forecast_lstm(lstm_model, 1, X)\n",
    "\t# invert scaling\n",
    "\tyhat = invert_scale(scaler, X, yhat)\n",
    "\t# invert differencing\n",
    "\tyhat = inverse_difference(raw_values, yhat, len(test_scaled)+1-i)\n",
    "\t# store forecast\n",
    "\tpredictions.append(yhat)\n",
    "\texpected = raw_values[len(train) + i + 1]\n",
    "\tprint('jour=%d, Predicted=%f, Expected=%f' % (i+1, yhat, expected))\n",
    " \n",
    "# report performance\n",
    "rmse = sqrt(mean_squared_error(raw_values[-12:], predictions))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "# line plot of observed vs predicted\n",
    "pyplot.plot(raw_values[-12:])\n",
    "pyplot.plot(predictions)\n",
    "pyplot.show()\n",
    "\n",
    "temp = cleaned_data.groupby('Date')['Recovered', 'Deaths', 'Active'].sum().reset_index()\n",
    "temp = temp.melt(id_vars=\"Date\", value_vars=['Recovered', 'Deaths', 'Active'],\n",
    "                 var_name='case', value_name='count')\n",
    "\n",
    "\n",
    "fig = px.area(temp, x=\"Date\", y=\"count\", color='case',\n",
    "             title='Cases over time: Area Plot', color_discrete_sequence = ['cyan', 'red', 'orange'])\n",
    "fig.show()\n",
    "\n",
    "\n",
    "formated_gdf = cleaned_data.groupby(['Date', 'Country/Region'])['Confirmed', 'Deaths', 'Active', 'Recovered'].max()\n",
    "formated_gdf = formated_gdf.reset_index()\n",
    "formated_gdf['Date'] = pd.to_datetime(formated_gdf['Date'])\n",
    "formated_gdf['Date'] = formated_gdf['Date'].dt.strftime('%m/%d/%Y')\n",
    "formated_gdf['size'] = formated_gdf['Active'].pow(0.3)\n",
    "formated_gdf['size'].fillna(formated_gdf['size'].mean(),inplace=True)\n",
    "\n",
    "fig = px.scatter_geo(formated_gdf, locations=\"Country/Region\", locationmode='country names', \n",
    "                     color=\"Active\", size='size', hover_name=\"Country/Region\", \n",
    "                     range_color= [0, 1000], \n",
    "                     projection=\"natural earth\", animation_frame=\"Date\", \n",
    "                     title='COVID-19: Active Cases Over Time', color_continuous_scale=\"portland\")\n",
    "fig.update(layout_coloraxis_showscale=False)\n",
    "fig.show()\n",
    "\n",
    "formated_gdf = cleaned_data.groupby(['Date', 'Country/Region'])['Confirmed', 'Deaths'].max()\n",
    "formated_gdf = formated_gdf.reset_index()\n",
    "formated_gdf['Date'] = pd.to_datetime(formated_gdf['Date'])\n",
    "formated_gdf['Date'] = formated_gdf['Date'].dt.strftime('%m/%d/%Y')\n",
    "formated_gdf['size'] = formated_gdf['Deaths'].pow(0.3)\n",
    "\n",
    "fig = px.scatter_geo(formated_gdf, locations=\"Country/Region\", locationmode='country names', \n",
    "                     color=\"Deaths\", size='size', hover_name=\"Country/Region\", \n",
    "                     range_color= [0, 100], \n",
    "                     projection=\"natural earth\", animation_frame=\"Date\", \n",
    "                     title='COVID-19: Deaths Over Time', color_continuous_scale=\"peach\")\n",
    "# fig.update(layout_coloraxis_showscale=False)\n",
    "fig.show()\n",
    "\n",
    "formated_gdf = cleaned_data.groupby(['Date', 'Country/Region'])['Confirmed', 'Deaths'].max()\n",
    "formated_gdf = formated_gdf.reset_index()\n",
    "formated_gdf['Date'] = pd.to_datetime(formated_gdf['Date'])\n",
    "formated_gdf['Date'] = formated_gdf['Date'].dt.strftime('%m/%d/%Y')\n",
    "formated_gdf['size'] = formated_gdf['Confirmed'].pow(0.3)\n",
    "\n",
    "fig = px.scatter_geo(formated_gdf, locations=\"Country/Region\", locationmode='country names', \n",
    "                     color=\"Confirmed\", size='size', hover_name=\"Country/Region\", \n",
    "                     range_color= [0, 1500], \n",
    "                     projection=\"natural earth\", animation_frame=\"Date\", \n",
    "                     title='COVID-19: Spread Over Time', color_continuous_scale=\"portland\")\n",
    "# fig.update(layout_coloraxis_showscale=False)\n",
    "fig.show()\n",
    "\n",
    "\n",
    "cleaned_latest = cleaned_data[cleaned_data['Date'] == max(cleaned_data['Date'])]\n",
    "flg = cleaned_latest.groupby('Country/Region')['Confirmed', 'Deaths', 'Recovered', 'Active'].sum().reset_index()\n",
    "\n",
    "flg['mortalityRate'] = round((flg['Deaths']/flg['Confirmed'])*100, 2)\n",
    "temp = flg[flg['Confirmed']>100]\n",
    "temp = temp.sort_values('mortalityRate', ascending=False)\n",
    "\n",
    "fig = px.bar(temp.sort_values(by=\"mortalityRate\", ascending=False)[:10][::-1],\n",
    "             x = 'mortalityRate', y = 'Country/Region', \n",
    "             title='Deaths per 100 Confirmed Cases', text='mortalityRate', height=800, orientation='h',\n",
    "             color_discrete_sequence=['darkred']\n",
    "            )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "from plotly.subplots import make_subplots\n",
    "import folium \n",
    "from folium import plugins\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "data_dir = Path('../input/covid19-global-forecasting-week-1')\n",
    "\n",
    "import os\n",
    "os.listdir(data_dir)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "fig = make_subplots(rows=3, cols=1)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(70), mode='lines+markers', y=train_dataset.loc[0].values, marker=dict(color=\"dodgerblue\"), showlegend=False,),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[0].values, mode='lines+markers', marker=dict(color=\"darkorange\"), showlegend=False,),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(70), mode='lines+markers', y=train_dataset.loc[1].values, marker=dict(color=\"dodgerblue\"), showlegend=False),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[1].values, mode='lines+markers', marker=dict(color=\"darkorange\"), showlegend=False),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(70), mode='lines+markers', y=train_dataset.loc[2].values, marker=dict(color=\"dodgerblue\"), showlegend=False),\n",
    "    row=3, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[2].values, mode='lines+markers', marker=dict(color=\"darkorange\"), showlegend=False),\n",
    "    row=3, col=1\n",
    ")\n",
    "\n",
    "fig.update_layout(height=1200, width=800, title_text=\"Train (blue) vs. Validation (orange) sales\")\n",
    "fig.show()\n",
    "\n",
    "predictions = []\n",
    "for i in range(len(val_dataset.columns)):\n",
    "    if i == 0:\n",
    "        predictions.append(train_dataset[train_dataset.columns[-1]].values)\n",
    "    else:\n",
    "        predictions.append(val_dataset[val_dataset.columns[i-1]].values)\n",
    "    \n",
    "predictions = np.transpose(np.array([row.tolist() for row in predictions]))\n",
    "error_naive = np.linalg.norm(predictions[:3] - val_dataset.values[:3])/len(predictions[0])\n",
    "\n",
    "pred_1 = predictions[0]\n",
    "pred_2 = predictions[1]\n",
    "pred_3 = predictions[2]\n",
    "\n",
    "fig = make_subplots(rows=3, cols=1)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(70), mode='lines', y=train_dataset.loc[0].values, marker=dict(color=\"dodgerblue\"),\n",
    "               name=\"Train\"),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[0].values, mode='lines', marker=dict(color=\"darkorange\"),\n",
    "               name=\"Val\"),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(70, 100), y=pred_1, mode='lines', marker=dict(color=\"seagreen\"),\n",
    "               name=\"Pred\"),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(70), mode='lines', y=train_dataset.loc[1].values, marker=dict(color=\"dodgerblue\"), showlegend=False),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[1].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(70, 100), y=pred_2, mode='lines', marker=dict(color=\"seagreen\"), showlegend=False,\n",
    "               name=\"Denoised signal\"),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(70), mode='lines', y=train_dataset.loc[2].values, marker=dict(color=\"dodgerblue\"), showlegend=False),\n",
    "    row=3, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[2].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False),\n",
    "    row=3, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(70, 100), y=pred_3, mode='lines', marker=dict(color=\"seagreen\"), showlegend=False,\n",
    "               name=\"Denoised signal\"),\n",
    "    row=3, col=1\n",
    ")\n",
    "\n",
    "fig.update_layout(height=1200, width=800, title_text=\"Naive approach\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARIMA\n",
    "\n",
    "Autoregressive integrated moving average (ARIMA) models were popularised by Box and Jenkins (1970). An ARIMA model describes a univariate time series as a combination of autoregressive (AR) and moving average (MA) lags which capture the autocorrelation within the time series. The order of integration denotes how many times the series has been differenced to obtain a stationary series.\n",
    "\n",
    "We write an ARIMA(p,d,q) model for some time series data yt, where p is the number of autoregressive lags, d is the degree of differencing and q is the number of moving average lags as:\n",
    "\n",
    "ARIMA models are associated with a Box-Jenkins approach to time series. According to this approach, you should difference the series until it is stationary, and then use information criteria and autocorrelation plots to choose the appropriate lag order for an ARIMA process. You then apply inference to obtain latent variable estimates, and check the model to see whether the model has captured the autocorrelation in the time series. For example, you can plot the autocorrelation of the model residuals. Once you are happy, you can use the model for retrospection and forecasting.\n",
    "\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "\n",
    "predictions = []\n",
    "for row in tqdm(train_dataset[train_dataset.columns[-30:]].values[:3]):\n",
    "    fit = sm.tsa.statespace.SARIMAX(row, seasonal_order=(0, 1, 1, 7)).fit()\n",
    "    predictions.append(fit.forecast(30))\n",
    "predictions = np.array(predictions).reshape((-1, 30))\n",
    "error_arima = np.linalg.norm(predictions[:3] - val_dataset.values[:3])/len(predictions[0])\n",
    "\n",
    "pred_1 = predictions[0]\n",
    "pred_2 = predictions[1]\n",
    "pred_3 = predictions[2]\n",
    "\n",
    "fig = make_subplots(rows=3, cols=1)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(70), mode='lines', y=train_dataset.loc[0].values, marker=dict(color=\"dodgerblue\"),\n",
    "               name=\"Train\"),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[0].values, mode='lines', marker=dict(color=\"darkorange\"),\n",
    "               name=\"Val\"),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(70, 100), y=pred_1, mode='lines', marker=dict(color=\"seagreen\"),\n",
    "               name=\"Pred\"),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(70), mode='lines', y=train_dataset.loc[1].values, marker=dict(color=\"dodgerblue\"), showlegend=False),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[1].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(70, 100), y=pred_2, mode='lines', marker=dict(color=\"seagreen\"), showlegend=False,\n",
    "               name=\"Denoised signal\"),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(70), mode='lines', y=train_dataset.loc[2].values, marker=dict(color=\"dodgerblue\"), showlegend=False),\n",
    "    row=3, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[2].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False),\n",
    "    row=3, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(70, 100), y=pred_3, mode='lines', marker=dict(color=\"seagreen\"), showlegend=False,\n",
    "               name=\"Denoised signal\"),\n",
    "    row=3, col=1\n",
    ")\n",
    "\n",
    "fig.update_layout(height=1200, width=800, title_text=\"ARIMA\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
